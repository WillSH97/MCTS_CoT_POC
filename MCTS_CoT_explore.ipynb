{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e403983-7020-44c1-998d-0b73703ffe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfc086d3-e0d8-4949-922b-09af37b5d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18817416-8ee9-433c-b098-4fa157bf2c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab208c2a-d096-454c-9187-4a0267b55bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.21s/it]\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "                                           quantization_config=quantization_config, \n",
    "                                           torch_dtype=torch.float32, \n",
    "                                           device_map=torch_device)\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3d49385-a7ec-4a70-9338-1f5e7faa082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "tokenizer.pad_token_id=tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cd9a4b2-f7ef-4432-ba2d-5bd55ee86ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eor_token = 'This is the end of this step' #Use this for trying to get a regex match to terminate generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4898f927-7a6e-43c0-aba5-37f9affa6850",
   "metadata": {},
   "outputs": [],
   "source": [
    "## KNOWLEDGE TEMPLATE\n",
    "\n",
    "def knowledge_message_template(textinput: str):\n",
    "\n",
    "    knowledge_messages = [\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": f\"\"\"\n",
    "                You are a helpful chatbot who will always give the correct answer, and use chain of thought reasoning to think step by step through complicated problems. \n",
    "                Consider what information you would need to find, what operations you would need to solve the problem, and give reasoning for each step. \n",
    "                Make each step the smallest logical unit possible, and make each step 128 words or less.\n",
    "                \"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"{textinput}\"},\n",
    "    ]\n",
    "\n",
    "    return knowledge_messages\n",
    "\n",
    "    ### other stuff I tried in the system chat head for knowledge message\n",
    "    \n",
    "        # , and output a {eor_token} at the END of each step of reasoning. Make your outputs for each step incredibly short. If you do not output a {eor_token}, you will be penalized heavily.\n",
    "    \n",
    "        #         Format your logic as follows:\n",
    "    \n",
    "        #         STEP 1: reasoning... {eor_token}\n",
    "        #         STEP 2: reasoning... {eor_token}\n",
    "        #         ...\n",
    "        #         STEP n: reasoning... {eor_token}\n",
    "    \n",
    "        #         \"\"\",\n",
    "    \n",
    "    \n",
    "                # For example, if asked the following:\n",
    "                \n",
    "                # Question:\n",
    "                # 'If there are 10 birds on a branch, and 3 fly away, how many birds are left?'\n",
    "    \n",
    "                # Answer:\n",
    "                # 1. There are 10 birds initially, and 3 fly away, which means we have 10-3 birds left on the branch. {EndOfChain}\n",
    "                # 2. 10 - 3 = 7. {EndOfChain}\n",
    "                # 3. Therefore, there are 7 birds left on the branch. {EndOfChain}\n",
    "\n",
    "def reasoning_message_template(question: str, logic: str):\n",
    "\n",
    "    reasoning_messages = [\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": f\"\"\"\n",
    "               You are a critical chatbot who is supposed to evaluate how strong the reasoning and logic is of a user's input. They will always give you a question and some logic, reasoning, and potentially some maths, and you have to rate it on a scale of 1-100 for how logical it is, with 10 being the most\n",
    "               logical and correct, and 1 being the least. Here are some examples:\n",
    "\n",
    "               ----\n",
    "\n",
    "               USER: Question: If I walked at 5km/h and my destination was 15km away, how long would it take me to walk to my destination and back?\n",
    "\n",
    "               Reasoning: To solve this, I need to know how far I am walking, and the speed of my walk:\n",
    "\n",
    "               Step 1: determine how far I am walking. Distance to destination is 15km, and I am walking back to where I started. Therefore, I am walking 15km there, and 15km back.\n",
    "               Therefore, distance walked is 15km*2 = 30km\n",
    "\n",
    "               Step 2: I know I am walking 5km/h, and I need to walk 30km. 30km divided by 5km/h is 30/5=6.\n",
    "               \n",
    "               Therefore it would take me 6 hours to walk to my destination and back\n",
    "\n",
    "               SYSTEM: This answer is a 100 out of 100 in reasoning and logic, since there are no logical flaws in its statements.\n",
    "\n",
    "\n",
    "               ----\n",
    "\n",
    "               USER: Question: if I have 10 apples and I eat 5, how many apples do I have left to eat?\n",
    "\n",
    "               Reasoning: To solve this, I need to know how many apples I started with, and how many I ate, and then subtract the number I ate from the number I started with.\n",
    "\n",
    "               Step 1: I have 10 apples, and I eat 5, then I subtract 5 from 10. 10 - 5 = 7.\n",
    "\n",
    "               Therefore, I have 7 apples left\n",
    "\n",
    "               SYSTEM: this answer is a 40 out of 10 because, even though the reasoning is correct, the maths is wrong and therefore the answer is wrong.\n",
    "\n",
    "               ----\n",
    "\n",
    "               USER: Question: What is the price of 10 beans, if 20 oranges cost $20, and each bean costs 1/3 of the price of an orange?\n",
    "\n",
    "               Reasoning: To solve this, I need to work out the cost of one bean, and then mulitply by the number of beans.\n",
    "\n",
    "               Step 1: the cost of one bean is $20 * 1/3, because a bean costs 1/3 of the price of an orange. $20 * 1/3 = $6.238176\n",
    "\n",
    "               Step 2: multiply the cost of one bean by 10 to get the cost of 10 beans. $6.238176 * 10 = $523.893284\n",
    "\n",
    "               Therefore the price of 10 beans is $523.893284\n",
    "\n",
    "               SYSTEM: This answer is a 1 out of 10 because there are multiple logical errors, and multiple arithmetic errors.\n",
    "\n",
    "               ---\n",
    "\n",
    "               Now rate the following:\n",
    "\n",
    "                \"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "            Question: {question}\n",
    "            Reasoning: {logic}\n",
    "        \n",
    "        \"\"\"},\n",
    "    ]\n",
    "    return reasoning_messages\n",
    "\n",
    "def tokenize_message(message):\n",
    "    inputs_chattemp = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(inputs_chattemp, return_tensors='pt').to(torch_device)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00f735db-f147-4434-9597-794631d63296",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'If I ran around a rectangular park twice, where the longer side is 123 metres and the shorter side is 45 metres, how far did I run, and what is the area of the park?'\n",
    "\n",
    "good_logic = \"\"\"\n",
    "To answer these questions, I need to find out the perimeter of the park, and then double it to find out the distance I ran, and then separately I must also multiply the two sides to get the area.\n",
    "\n",
    "Step 1: The longer side of the park is 123 metres, and the shorter side is 45 metres. The perimeter is 2*123m + 2*45m = 336m.\n",
    "\n",
    "Step 2: I ran around the park twice. Twice the perimeter = 2*336m = 672m\n",
    "\n",
    "Therefore I ran 672m.\n",
    "\n",
    "Step 3: To get the area, I must multiply both sides. 123m * 45m = 5535m2\n",
    "\n",
    "Therefore, I ran 672m, and the area of the park is 5535m2\n",
    "\"\"\"\n",
    "\n",
    "bad_logic = \"\"\"\n",
    "To answer these questions, I need to find out the distance ran, and the area.\n",
    "\n",
    "Step 1: The distance ran is twice the perimeter. The perimeter = 123m + 45m = 168m. Therefore the distance ran is 168m * 2 = 302m.\n",
    "\n",
    "Step 2: the area of the park is equal to perimeter^2. Perimeter is 302m^2 = 8172m2.\n",
    "\n",
    "Therefore I ran 302m, and the area of the park is 8172m2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f4cf524-8fb2-44f1-ac09-dd9f11bcc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Yes or no: Would a pear sink in water?'\n",
    "\n",
    "good_logic = \"\"\"\n",
    "The density of a pear is about 0.6 g/cm^3, which is less than water. Thus, a pear would float. Therefore, the answer is no.\n",
    "\"\"\"\n",
    "\n",
    "bad_logic = \"\"\"\n",
    "We do not have enough information to answer the question, but since most pears are not filled with air, the pear would sink. Therefore, the answer is yes.\n",
    "\"\"\"\n",
    "\n",
    "incomplete_logic = \"\"\"\n",
    "To determine if a pear would sink in water, we do not have enough information.\n",
    "\n",
    "Most pears have no air in them\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e6b53d0-b92b-4c13-b275-8dbb7f0212a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERIC PIPELINE\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1024, do_sample = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "618f9da7-231d-4951-8671-3496ba9e8c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This answer is a 10 out of 100 in reasoning and logic. \n",
      "\n",
      "The reasoning is correct in stating that we don't have enough information to determine if a pear would sink in water. However, the conclusion that most pears have no air in them is irrelevant to the question of whether a pear would sink in water. The presence or absence of air in a pear does not affect its density or buoyancy in water. Therefore, the answer should focus on the fact that we don't have enough information to make a conclusion, rather than making a statement about the pear's composition.\n"
     ]
    }
   ],
   "source": [
    "results = pipe(reasoning_message_template(question, incomplete_logic), temperature = 0.1)\n",
    "\n",
    "print(results[-1]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38fc0d8e-2f43-4515-939f-0763311559e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '''\n",
    "This answer is a 10 out of 10 in terms of reasoning and logic. \n",
    "\n",
    "The reasoning is partially correct, as pears do not have air pockets like some other objects, which could affect their buoyancy. However, the answer is incomplete and doesn't provide a clear conclusion. It also doesn't consider other factors that could influence whether a pear would sink in water, such as its density relative to water. Therefore, the answer is not fully logical or conclusive.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33929e76-6e5d-43fc-849e-06aeab6e13a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regex matching for score out of 10\n",
    "re.search('([\\d]+)(?=\\s+out of 10)', test).group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfbbc44a-49b8-47c9-8419-2042858fef3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine if a pear would sink in water, we need to consider the density of a pear compared to water.\n",
      "\n",
      "Step 1: Identify the densities of a pear and water.\n",
      "The density of a pear is approximately 0.89 g/cm³.\n",
      "The density of water is approximately 1.00 g/cm³.\n",
      "\n",
      "Step 2: Compare the densities.\n",
      "Since the density of a pear (0.89 g/cm³) is less than the density of water (1.00 g/cm³), a pear would likely float in water.\n",
      "\n",
      "Answer: No\n"
     ]
    }
   ],
   "source": [
    "# generic non-MCTS chain of thought reasoning\n",
    "\n",
    "results2 = pipe(knowledge_message_template(question), temperature = 0.1)\n",
    "\n",
    "print(results2[-1]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9b66b304-3ad0-49bb-b09b-f5d2353c9f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:17<00:00, 13.72s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [06:47<00:00, 40.72s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:07<00:00, 12.76s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:17<00:00, 13.77s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:25<00:00, 14.59s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [21:24<00:00, 42.81s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:27<00:00, 14.71s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:22<00:00, 14.20s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:44<00:00, 16.44s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [30:54<00:00, 61.83s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:35<00:00,  9.56s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [19:20<00:00, 116.09s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:20<00:00, 14.09s/it]\n",
      " 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                          | 20/30 [11:11<05:35, 33.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping generation and iteration because we have more than 3 final outputs - we have 10 complete solutions to examine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [08:11<00:00, 49.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful chatbot who will always give the correct answer, and use chain of thought reasoning to think step by step through complicated problems. \n",
      "                Consider what information you would need to find, what operations you would need to solve the problem, and give reasoning for each step. \n",
      "                Make each step the smallest logical unit possible, and make each step 128 words or less.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Yes or no: Would a pear sink in water?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "To determine if a pear will sink in water, we need to consider its density. Density is the ratio of an object's mass to its volume. If the density of the pear is greater than that of water, it will sink. If the density is less than that of water, it will float. \n",
      "\n",
      "To find the density of a pear, we need to know its mass and volume. We can find these values by measuring the mass and volume of a specific pear. \n",
      "\n",
      "Here's a step-by-step process to determine if a pear will sink in water:\n",
      "\n",
      "1. Measure the mass of the pear (e.g., 0.3 kg).\n",
      "2. Measure the volume of the pear (e.g., 0.35 liters).\n",
      "3. Calculate the density of the pear (0.3 kg / 0.35 liters = 0.857 kg/liter).\n",
      "4. Compare the density of the pear (0.857 kg/liter) to that of water (1 kg/liter).\n",
      "5. Since the density of the pear is greater than that of water, the pear will sink.\n",
      "\n",
      "Therefore, yes, a pear will sink in water.<|eot_id|>\n",
      "~*~*~*~*~*~*~*~*~*~*~*~*~\n",
      "Time spent thinking: 0:42:40.337379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# MTCS chain of thought using only strict token limits\n",
    "\n",
    "question = 'Yes or no: Would a pear sink in water?'\n",
    "\n",
    "inputs_cot = tokenize_message(knowledge_message_template(question))\n",
    "\n",
    "\n",
    "num_branches = 10\n",
    "\n",
    "num_candidates = 3\n",
    "\n",
    "max_iters = 6\n",
    "\n",
    "current_iter = 0\n",
    "\n",
    "final_branches = []\n",
    "\n",
    "running_outputs = []\n",
    "\n",
    "# Conditions to stop generating: if minimum number of final candidates met, if max iters hit, \n",
    "generating = True\n",
    "\n",
    "while generating:\n",
    "    starttime = datetime.now()\n",
    "    \n",
    "    if current_iter == 0:\n",
    "        inputs_cot = [tokenize_message(knowledge_message_template(question))]\n",
    "        branches_list=[]\n",
    "    else:\n",
    "        inputs_cot = [tokenizer(branch, return_tensors='pt').to(torch_device) for branch in branches_list]\n",
    "\n",
    "    #save interim text outputs:\n",
    "    running_outputs.append(branches_list)\n",
    "\n",
    "    #re-init all interim variables\n",
    "    branches_list = []\n",
    "\n",
    "    logic_scores = []\n",
    "\n",
    "\n",
    "    # generate initial number of branches\n",
    "    for candidate in inputs_cot:\n",
    "        for i in tqdm(range(num_branches)):\n",
    "            branchoutput = model.generate(**candidate, max_new_tokens=64, temperature=0.5).cpu()\n",
    "            output_text = tokenizer.batch_decode(branchoutput)[0]\n",
    "            branches_list.append(output_text)\n",
    "    \n",
    "    #evaluate logic\n",
    "    for output in tqdm(branches_list):\n",
    "        #clean outputs\n",
    "        chat_list = output.split('<|eot_id|>')\n",
    "        # ADD LOGIC HERE TO GET GENERATION TO STOP AFTER THERE'S TWO \n",
    "    \n",
    "        # MAKE THE LOGIC THAT WE MUST HIT MINIMUM NUMBER OF NUM_CANDIDATES BEFORE STOPPING GENERATING\n",
    "        if len(chat_list) > 3:\n",
    "            final_branches.append(output)\n",
    "            branches_list.remove(output)\n",
    "            continue\n",
    "        \n",
    "        # isolating the output logic\n",
    "        logic = chat_list[2].replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\", \"\")\n",
    "        results = pipe(reasoning_message_template(question, logic), temperature = 0.1)\n",
    "        results_text = results[-1]['generated_text'][-1]['content']\n",
    "        #jank error handling for weird generations\n",
    "        try:\n",
    "            results_score = int(re.search('([\\d]+)(?=\\s+out of 10)', results_text).group(0))\n",
    "        except:\n",
    "            print(f\"no valid string match for {output} results - the output was: \\n {results_text}\")\n",
    "            results_score = 0\n",
    "            # NOTE: this is potentially getting rid of valid and good responses, but I'm being super lazy to get an MVP loop working\n",
    "            continue\n",
    "    \n",
    "        logic_scores.append(results_score)\n",
    "\n",
    "    \n",
    "    # if we reach the\n",
    "    if len(final_branches) > num_candidates:\n",
    "        generating = False\n",
    "        print(f\"stopping generation and iteration because we have more than {num_candidates} final outputs - we have {len(final_branches)} complete solutions to examine\")\n",
    "        final_branch_scores = []\n",
    "        for solution in tqdm(final_branches):\n",
    "            chat_list = output.split('<|eot_id|>')\n",
    "            logic = chat_list[2].replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\", \"\")\n",
    "            results = pipe(reasoning_message_template(question, logic), temperature = 0.1)\n",
    "            results_text = results[-1]['generated_text'][-1]['content']\n",
    "            try:\n",
    "                results_score = int(re.search('([\\d]+)(?=\\s+out of 10)', results_text).group(0))\n",
    "            except:\n",
    "                print(f\"no valid string match for {output} results - the output was: \\n {results_text}\")\n",
    "                results_score = 0\n",
    "                # NOTE: this is potentially getting rid of valid and good responses, but I'm being super lazy to get an MVP loop working\n",
    "                continue\n",
    "            final_branch_scores.append(results_score)\n",
    "\n",
    "        selection = np.argmax(final_branch_scores)\n",
    "\n",
    "        final_output = final_branches[selection]\n",
    "        print(final_output)\n",
    "        print(\"~*~*~*~*~*~*~*~*~*~*~*~*~\")\n",
    "        print(f\"Time spent thinking: {datetime.now()-starttime}\")\n",
    "\n",
    "    elif current_iter >= max_iters:\n",
    "        generating = False\n",
    "        print(f\"stopping generation and iteration - we have exceeded {max_iters} iterations. We have {len(final_branches)} complete solutions and {len(branches_list)} dangling branches to examine.\")\n",
    "        final_branches = final_branches.extend(branches_list)\n",
    "        final_branch_scores = []\n",
    "        for solution in tqdm(final_branches):\n",
    "            chat_list = output.split('<|eot_id|>')\n",
    "            logic = chat_list[2].replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\", \"\")\n",
    "            results = pipe(reasoning_message_template(question, logic), temperature = 0.1)\n",
    "            results_text = results[-1]['generated_text'][-1]['content']\n",
    "            try:\n",
    "                results_score = int(re.search('([\\d]+)(?=\\s+out of 10)', results_text).group(0))\n",
    "            except:\n",
    "                print(f\"no valid string match for {output} results - the output was: \\n {results_text}\")\n",
    "                results_score = 0\n",
    "                # NOTE: this is potentially getting rid of valid and good responses, but I'm being super lazy to get an MVP loop working\n",
    "                continue\n",
    "            final_branch_scores.append(results_score)\n",
    "\n",
    "        selection = np.argmax(final_branch_scores)\n",
    "\n",
    "        final_output = final_branches[selection]\n",
    "        print(final_output)\n",
    "        print(\"~*~*~*~*~*~*~*~*~*~*~*~*~\")\n",
    "        print(f\"Time spent thinking: {datetime.now()-starttime}\")\n",
    "    \n",
    "    else:\n",
    "        probs = softmax(logic_scores).tolist()\n",
    "        chosen_keys = np.random.choice(len(probs), size=num_candidates, replace = False, p = probs).tolist()\n",
    "        branches_list = [branches_list[index] for index in chosen_keys]\n",
    "        current_iter +=1\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625eee63-9134-43f5-9d42-bcb2801e7d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RF-demos",
   "language": "python",
   "name": "rf-demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
